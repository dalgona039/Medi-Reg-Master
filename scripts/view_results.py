#!/usr/bin/env python3
"""
ê²°ê³¼ ìš”ì•½ ë·°ì–´ - JSON íŒŒì‹± ì—†ì´ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ì¶œë ¥
"""

import json
import sys
from pathlib import Path
from typing import Dict, Any


def format_percentage(value: float) -> str:
    """í¼ì„¼íŠ¸ í˜•ì‹ìœ¼ë¡œ í¬ë§·"""
    return f"{value * 100:.1f}%"


def format_number(value: float, precision: int = 3) -> str:
    """ìˆ«ì í¬ë§·"""
    return f"{value:.{precision}f}"


def get_winner_emoji(winner: str) -> str:
    """ìŠ¹ì ì´ëª¨ì§€"""
    if winner == "TreeRAG":
        return "ğŸŒ³ TreeRAG"
    elif winner == "FlatRAG":
        return "ğŸ“„ FlatRAG"
    elif winner == "BM25":
        return "ğŸ” BM25"
    return "ğŸ¤ ë¬´ìŠ¹ë¶€"


def print_header(text: str):
    """í—¤ë” ì¶œë ¥"""
    print("\n" + "=" * 70)
    print(f"  {text}")
    print("=" * 70)


def print_subheader(text: str):
    """ì„œë¸Œí—¤ë” ì¶œë ¥"""
    print(f"\nğŸ“Š {text}")
    print("-" * 70)


def view_comparison_results(results_path: Path):
    """ë¹„êµ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    
    if not results_path.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {results_path}")
        return
    
    with open(results_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print_header("ğŸ¯ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
    
    # ì‹œìŠ¤í…œ ì •ë³´
    systems = data.get("systems", {})
    primary = systems.get("primary", "System A")
    baseline = systems.get("baseline", "System B")
    
    print(f"\në¹„êµ ì‹œìŠ¤í…œ:")
    print(f"  ğŸŸ¢ ì£¼ìš” ì‹œìŠ¤í…œ: {primary}")
    print(f"  ğŸ”µ ê¸°ì¤€ ì‹œìŠ¤í…œ: {baseline}")
    
    # ì „ì²´ ìš”ì•½
    summary = data.get("summary", {})
    print_subheader("ì „ì²´ ìŠ¹íŒ¨")
    print(f"  {primary} ìŠ¹ë¦¬:  {summary.get(f'{primary}_wins', 0)}ê°œ ë©”íŠ¸ë¦­")
    print(f"  {baseline} ìŠ¹ë¦¬:  {summary.get(f'{baseline}_wins', 0)}ê°œ ë©”íŠ¸ë¦­")
    print(f"  ë¬´ìŠ¹ë¶€:         {summary.get('ties', 0)}ê°œ ë©”íŠ¸ë¦­")
    print(f"  ì „ì²´ ë¹„êµ:      {summary.get('total_comparisons', 0)}ê°œ ë©”íŠ¸ë¦­")
    
    # ìƒì„¸ ë¹„êµ
    comparisons = data.get("comparisons", {})
    
    print_subheader("ê²€ìƒ‰ ì„±ëŠ¥ (Retrieval Metrics)")
    retrieval_metrics = ["P@1", "P@3", "P@5", "P@10", "NDCG@1", "NDCG@3", "NDCG@5", "NDCG@10", "MRR"]
    
    for metric in retrieval_metrics:
        if metric in comparisons:
            comp = comparisons[metric]
            winner = comp.get("winner")
            primary_val = comp.get(f"{primary}_mean", 0)
            baseline_val = comp.get(f"{baseline}_mean", 0)
            p_value = comp.get("p_value", 1.0)
            effect_size = comp.get("effect_size", 0)
            
            # í†µê³„ì  ìœ ì˜ì„±
            sig_marker = "âœ“" if p_value < 0.05 else "âœ—"
            
            # íš¨ê³¼ í¬ê¸° í•´ì„
            effect_interp = comp.get("effect_interpretation", "negligible")
            
            winner_emoji = get_winner_emoji(winner) if winner else "ğŸ¤ ë¬´ìŠ¹ë¶€"
            
            print(f"\n  {metric:10s}  {winner_emoji}")
            print(f"    {primary:10s}: {format_percentage(primary_val):>6s}")
            print(f"    {baseline:10s}: {format_percentage(baseline_val):>6s}")
            print(f"    ì°¨ì´: {format_percentage(abs(primary_val - baseline_val)):>6s}  |  p-value: {p_value:.4f} {sig_marker}  |  íš¨ê³¼: {effect_interp}")
    
    print_subheader("íš¨ìœ¨ì„± (Efficiency Metrics)")
    efficiency_metrics = ["Latency (ms)", "Tokens", "Nodes Visited"]
    
    for metric in efficiency_metrics:
        if metric in comparisons:
            comp = comparisons[metric]
            winner = comp.get("winner")
            primary_val = comp.get(f"{primary}_mean", 0)
            baseline_val = comp.get(f"{baseline}_mean", 0)
            p_value = comp.get("p_value", 1.0)
            
            sig_marker = "âœ“" if p_value < 0.05 else "âœ—"
            winner_emoji = get_winner_emoji(winner) if winner else "ğŸ¤ ë¬´ìŠ¹ë¶€"
            
            print(f"\n  {metric:15s}  {winner_emoji}")
            print(f"    {primary:10s}: {format_number(primary_val):>8s}")
            print(f"    {baseline:10s}: {format_number(baseline_val):>8s}")
            print(f"    ì°¨ì´: {format_number(abs(primary_val - baseline_val)):>8s}  |  p-value: {p_value:.4f} {sig_marker}")
    
    print_subheader("ì‹ ë¢°ë„ (Fidelity Metrics)")
    fidelity_metrics = ["Groundedness", "Hallucination Rate", "Citation Accuracy"]
    
    for metric in fidelity_metrics:
        if metric in comparisons:
            comp = comparisons[metric]
            winner = comp.get("winner")
            primary_val = comp.get(f"{primary}_mean", 0)
            baseline_val = comp.get(f"{baseline}_mean", 0)
            p_value = comp.get("p_value", 1.0)
            
            sig_marker = "âœ“" if p_value < 0.05 else "âœ—"
            winner_emoji = get_winner_emoji(winner) if winner else "ğŸ¤ ë¬´ìŠ¹ë¶€"
            
            print(f"\n  {metric:20s}  {winner_emoji}")
            print(f"    {primary:10s}: {format_percentage(primary_val):>6s}")
            print(f"    {baseline:10s}: {format_percentage(baseline_val):>6s}")
            print(f"    ì°¨ì´: {format_percentage(abs(primary_val - baseline_val)):>6s}  |  p-value: {p_value:.4f} {sig_marker}")
    
    # ê²°ë¡ 
    print_header("ğŸ“ ê²°ë¡ ")
    
    if summary.get(f'{primary}_wins', 0) > summary.get(f'{baseline}_wins', 0):
        print(f"\n  ğŸ‰ {primary}ê°€ {baseline}ë³´ë‹¤ ìš°ìˆ˜í•©ë‹ˆë‹¤!")
    elif summary.get(f'{primary}_wins', 0) < summary.get(f'{baseline}_wins', 0):
        print(f"\n  âš ï¸  {baseline}ê°€ {primary}ë³´ë‹¤ ìš°ìˆ˜í•©ë‹ˆë‹¤.")
    else:
        print(f"\n  ğŸ¤ {primary}ì™€ {baseline}ê°€ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.")
    
    print("\n  ğŸ’¡ í†µê³„ì  ìœ ì˜ì„± (p-value < 0.05)ì´ ìˆëŠ” ì°¨ì´ë§Œ ì˜ë¯¸ ìˆìŠµë‹ˆë‹¤.")
    print("\n" + "=" * 70 + "\n")


def view_evaluation_report(report_path: Path):
    """ì „ì²´ í‰ê°€ ê²°ê³¼ ìš”ì•½"""
    
    if not report_path.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {report_path}")
        return
    
    with open(report_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print_header("ğŸ“Š ì „ì²´ í‰ê°€ ê²°ê³¼")
    
    experiment = data.get("experiment", "Unknown")
    timestamp = data.get("timestamp", "Unknown")
    systems = data.get("systems", {})
    
    print(f"\nì‹¤í—˜ëª…: {experiment}")
    print(f"ì‹¤í–‰ ì‹œê°„: {timestamp}")
    print(f"í‰ê°€ ì‹œìŠ¤í…œ: {', '.join(systems.keys())}")
    
    for system_name, system_data in systems.items():
        print_subheader(f"ì‹œìŠ¤í…œ: {system_name}")
        
        retrieval = system_data.get("retrieval_metrics", {})
        if retrieval:
            print("\n  ê²€ìƒ‰ ì„±ëŠ¥:")
            for metric, value in retrieval.items():
                if isinstance(value, dict):
                    for k, v in value.items():
                        print(f"    {metric}@{k}: {format_percentage(v)}")
                elif isinstance(value, (int, float)):
                    print(f"    {metric}: {format_percentage(value)}")
        
        efficiency = system_data.get("efficiency_metrics", {})
        if efficiency:
            print("\n  íš¨ìœ¨ì„±:")
            avg_latency = efficiency.get("avg_latency_ms", 0)
            avg_tokens = efficiency.get("avg_tokens", 0)
            print(f"    í‰ê·  ì§€ì—°ì‹œê°„: {avg_latency:.2f}ms")
            print(f"    í‰ê·  í† í°: {avg_tokens:.0f}")
        
        fidelity = system_data.get("fidelity_metrics", {})
        if fidelity:
            print("\n  ì‹ ë¢°ë„:")
            groundedness = fidelity.get("avg_groundedness", 0)
            print(f"    í‰ê·  ê·¼ê±°ì„±: {format_percentage(groundedness)}")
    
    print("\n" + "=" * 70 + "\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    if len(sys.argv) > 1:
        results_path = Path(sys.argv[1])
    else:
        # ê¸°ë³¸ ê²½ë¡œ ì°¾ê¸°
        default_comparison = Path("benchmarks/results/default/treerag_vs_flatrag/comparison_report.json")
        default_evaluation = Path("benchmarks/results/default/evaluation_report.json")
        
        if default_comparison.exists():
            results_path = default_comparison
        elif default_evaluation.exists():
            results_path = default_evaluation
        else:
            print("âŒ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("\nì‚¬ìš©ë²•:")
            print("  python scripts/view_results.py [ê²°ê³¼_íŒŒì¼_ê²½ë¡œ]")
            print("\nì˜ˆì‹œ:")
            print("  python scripts/view_results.py benchmarks/results/default/treerag_vs_flatrag/comparison_report.json")
            return
    
    # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ ë·°ì–´ í˜¸ì¶œ
    if "comparison" in results_path.name:
        view_comparison_results(results_path)
    elif "evaluation" in results_path.name:
        view_evaluation_report(results_path)
    else:
        print(f"âš ï¸  ì•Œ ìˆ˜ ì—†ëŠ” ê²°ê³¼ íŒŒì¼ í˜•ì‹: {results_path.name}")
        print("comparison_report.json ë˜ëŠ” evaluation_report.json íŒŒì¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()
