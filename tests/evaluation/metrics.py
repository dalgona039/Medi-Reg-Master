import time
import numpy as np
from typing import List, Dict, Any, Set, Tuple
from collections import defaultdict
import re


class EvaluationMetrics:
    """
    RAG ì‹œìŠ¤í…œ í‰ê°€ë¥¼ ìœ„í•œ ì¢…í•© ë©”íŠ¸ë¦­ í´ë˜ìŠ¤
    
    ì„ì‚¬ ì—°êµ¬ ê¸°ì¤€:
    - Retrieval: Precision@K, Recall@K, F1@K, NDCG@K
    - Generation: Faithfulness, Citation Accuracy
    - Efficiency: Latency, Token Reduction Rate
    """
    
    @staticmethod
    def precision_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:
        """
        Precision@K: ìƒìœ„ Kê°œ ì¤‘ ì •ë‹µ ë¹„ìœ¨
        
        Formula: P@K = |retrieved[:k] âˆ© relevant| / k
        
        Args:
            retrieved: ì‹œìŠ¤í…œì´ ë°˜í™˜í•œ ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸ (ìˆœì„œ ì¤‘ìš”)
            relevant: ì •ë‹µ ë¬¸ì„œ ID ì§‘í•©
            k: ìƒìœ„ ëª‡ ê°œë¥¼ ë³¼ ê²ƒì¸ê°€
            
        Returns:
            0.0 ~ 1.0 ì‚¬ì´ì˜ precision ê°’
            
        Example:
            retrieved = ['doc1', 'doc2', 'doc3', 'doc4', 'doc5']
            relevant = {'doc1', 'doc3', 'doc6'}
            precision_at_k(retrieved, relevant, 3) = 2/3 = 0.667
        """
        if k == 0:
            return 0.0
        
        retrieved_at_k = retrieved[:k]
        relevant_retrieved = sum(1 for doc in retrieved_at_k if doc in relevant)
        
        return relevant_retrieved / k
    
    @staticmethod
    def recall_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:
        """
        Recall@K: ì •ë‹µ ì¤‘ ìƒìœ„ Kê°œì—ì„œ ì°¾ì€ ë¹„ìœ¨
        
        Formula: R@K = |retrieved[:k] âˆ© relevant| / |relevant|
        
        Args:
            retrieved: ì‹œìŠ¤í…œì´ ë°˜í™˜í•œ ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸
            relevant: ì •ë‹µ ë¬¸ì„œ ID ì§‘í•©
            k: ìƒìœ„ ëª‡ ê°œë¥¼ ë³¼ ê²ƒì¸ê°€
            
        Returns:
            0.0 ~ 1.0 ì‚¬ì´ì˜ recall ê°’
            
        Example:
            retrieved = ['doc1', 'doc2', 'doc3']
            relevant = {'doc1', 'doc3', 'doc6'}
            recall_at_k(retrieved, relevant, 3) = 2/3 = 0.667
        """
        if len(relevant) == 0:
            return 0.0
        
        retrieved_at_k = retrieved[:k]
        relevant_retrieved = sum(1 for doc in retrieved_at_k if doc in relevant)
        
        return relevant_retrieved / len(relevant)
    
    @staticmethod
    def f1_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:
        """
        F1@K: Precisionê³¼ Recallì˜ ì¡°í™”í‰ê· 
        
        Formula: F1@K = 2 * (P@K * R@K) / (P@K + R@K)
        
        Args:
            retrieved: ì‹œìŠ¤í…œì´ ë°˜í™˜í•œ ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸
            relevant: ì •ë‹µ ë¬¸ì„œ ID ì§‘í•©
            k: ìƒìœ„ ëª‡ ê°œë¥¼ ë³¼ ê²ƒì¸ê°€
            
        Returns:
            0.0 ~ 1.0 ì‚¬ì´ì˜ F1 ê°’
        """
        precision = EvaluationMetrics.precision_at_k(retrieved, relevant, k)
        recall = EvaluationMetrics.recall_at_k(retrieved, relevant, k)
        
        if precision + recall == 0:
            return 0.0
        
        return 2 * (precision * recall) / (precision + recall)
    
    @staticmethod
    def ndcg_at_k(retrieved: List[str], relevant: Dict[str, float], k: int) -> float:
        """
        NDCG@K: Normalized Discounted Cumulative Gain
        
        ìˆœì„œë¥¼ ê³ ë ¤í•œ ë­í‚¹ í‰ê°€ (ìƒìœ„ì¼ìˆ˜ë¡ ì¤‘ìš”)
        
        Formula:
            DCG@K = Î£(rel_i / log2(i+1)) for i in 1..k
            NDCG@K = DCG@K / IDCG@K
        
        Args:
            retrieved: ì‹œìŠ¤í…œì´ ë°˜í™˜í•œ ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸
            relevant: {doc_id: relevance_score} í˜•íƒœì˜ ì •ë‹µ (0.0~1.0)
            k: ìƒìœ„ ëª‡ ê°œë¥¼ ë³¼ ê²ƒì¸ê°€
            
        Returns:
            0.0 ~ 1.0 ì‚¬ì´ì˜ NDCG ê°’
            
        Example:
            retrieved = ['doc1', 'doc2', 'doc3']
            relevant = {'doc1': 1.0, 'doc2': 0.5, 'doc4': 1.0}
            ndcg_at_k(retrieved, relevant, 3) â‰ˆ 0.85
        """
        if k == 0 or len(relevant) == 0:
            return 0.0
        
        # DCG ê³„ì‚°
        dcg = 0.0
        for i, doc_id in enumerate(retrieved[:k]):
            if doc_id in relevant:
                # i+2: ìˆœìœ„ëŠ” 1ë¶€í„° ì‹œì‘, log2(1)=0 ë°©ì§€
                dcg += relevant[doc_id] / np.log2(i + 2)
        
        # IDCG ê³„ì‚° (ì´ìƒì ì¸ ìˆœì„œë¡œ ì •ë ¬í–ˆì„ ë•Œ)
        ideal_scores = sorted(relevant.values(), reverse=True)[:k]
        idcg = sum(score / np.log2(i + 2) for i, score in enumerate(ideal_scores))
        
        if idcg == 0:
            return 0.0
        
        return dcg / idcg
    
    @staticmethod
    def mean_reciprocal_rank(retrieved_list: List[List[str]], relevant_list: List[Set[str]]) -> float:
        """
        MRR: Mean Reciprocal Rank
        
        ì²« ë²ˆì§¸ ì •ë‹µì´ ëª‡ ë²ˆì§¸ ìˆœìœ„ì— ë‚˜íƒ€ë‚˜ëŠ”ê°€?
        
        Formula: MRR = (1/|Q|) * Î£(1/rank_i)
        
        Args:
            retrieved_list: ê° ì¿¼ë¦¬ë³„ ë°˜í™˜ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë“¤
            relevant_list: ê° ì¿¼ë¦¬ë³„ ì •ë‹µ ì§‘í•©ë“¤
            
        Returns:
            0.0 ~ 1.0 ì‚¬ì´ì˜ MRR ê°’
            
        Example:
            retrieved_list = [
                ['doc1', 'doc2', 'doc3'],  # ì²« ì •ë‹µ ìœ„ì¹˜: 1 (rank=1)
                ['doc4', 'doc5', 'doc1'],  # ì²« ì •ë‹µ ìœ„ì¹˜: 3 (rank=3)
            ]
            relevant_list = [{'doc1', 'doc6'}, {'doc1', 'doc7'}]
            mrr = (1/1 + 1/3) / 2 = 0.667
        """
        if len(retrieved_list) != len(relevant_list):
            raise ValueError("retrieved_list and relevant_list must have same length")
        
        reciprocal_ranks = []
        
        for retrieved, relevant in zip(retrieved_list, relevant_list):
            for rank, doc_id in enumerate(retrieved, start=1):
                if doc_id in relevant:
                    reciprocal_ranks.append(1.0 / rank)
                    break
            else:
                # ì •ë‹µì„ ì°¾ì§€ ëª»í•œ ê²½ìš°
                reciprocal_ranks.append(0.0)
        
        return np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0
    
    @staticmethod
    def citation_accuracy(generated_answer: str, ground_truth_citations: Set[str]) -> Tuple[float, Dict[str, Any]]:
        """
        Citation Accuracy: ìƒì„±ëœ ë‹µë³€ì˜ ì¸ìš© ì •í™•ë„
        
        TreeRAGì˜ í•µì‹¬ ê°•ì ì¸ page-level citation í‰ê°€
        
        Args:
            generated_answer: ì‹œìŠ¤í…œì´ ìƒì„±í•œ ë‹µë³€
            ground_truth_citations: ì •ë‹µ ì¸ìš© ì§‘í•© (ì˜ˆ: {'doc1#p10', 'doc2#p5'})
            
        Returns:
            accuracy (float): 0.0~1.0
            details (dict): ì„¸ë¶€ ì •ë³´
            
        Example:
            answer = "ë‹µë³€ì…ë‹ˆë‹¤ [ë¬¸ì„œA, p.10] [ë¬¸ì„œB, p.5]"
            ground_truth = {'ë¬¸ì„œA#p10', 'ë¬¸ì„œB#p5', 'ë¬¸ì„œC#p3'}
            accuracy = 2/3 = 0.667 (2ê°œ ì •í™•íˆ ì¸ìš©, 1ê°œ ë†“ì¹¨)
        """
        # ë‹µë³€ì—ì„œ ì¸ìš© ì¶”ì¶œ íŒ¨í„´: [ë¬¸ì„œëª…, p.X] ë˜ëŠ” [Document, p.X]
        citation_pattern = r'\[([^,\]]+),\s*p\.(\d+)\]'
        found_citations = re.findall(citation_pattern, generated_answer)
        
        # ì¸ìš©ì„ 'doc_name#pX' í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”
        found_set = set()
        for doc_name, page_num in found_citations:
            doc_name = doc_name.strip()
            citation_key = f"{doc_name}#p{page_num}"
            found_set.add(citation_key)
        
        # ì •í™•ë„ ê³„ì‚°
        if len(ground_truth_citations) == 0:
            accuracy = 1.0 if len(found_set) == 0 else 0.0
        else:
            correct_citations = found_set.intersection(ground_truth_citations)
            accuracy = len(correct_citations) / len(ground_truth_citations)
        
        return accuracy, {
            'found_citations': list(found_set),
            'expected_citations': list(ground_truth_citations),
            'correct': len(found_set.intersection(ground_truth_citations)),
            'missing': len(ground_truth_citations - found_set),
            'extra': len(found_set - ground_truth_citations)
        }
    
    @staticmethod
    def context_reduction_rate(flat_context_size: int, tree_context_size: int) -> float:
        """
        Context Reduction Rate: TreeRAGì˜ ì»¨í…ìŠ¤íŠ¸ ê°ì†Œìœ¨
        
        TreeRAGì˜ í•µì‹¬ ì£¼ì¥: "90%+ ì»¨í…ìŠ¤íŠ¸ ê°ì†Œ"ë¥¼ ê²€ì¦
        
        Formula: reduction_rate = 1 - (tree_size / flat_size)
        
        Args:
            flat_context_size: Flat RAGì˜ ì»¨í…ìŠ¤íŠ¸ í¬ê¸° (tokens)
            tree_context_size: TreeRAGì˜ ì»¨í…ìŠ¤íŠ¸ í¬ê¸° (tokens)
            
        Returns:
            0.0 ~ 1.0 ì‚¬ì´ì˜ ê°ì†Œìœ¨ (0.9 = 90% ê°ì†Œ)
            
        Example:
            flat_size = 10000 tokens
            tree_size = 800 tokens
            reduction = 1 - (800/10000) = 0.92 (92% ê°ì†Œ)
        """
        if flat_context_size == 0:
            return 0.0
        
        reduction = 1.0 - (tree_context_size / flat_context_size)
        
        return max(0.0, reduction)  # ìŒìˆ˜ ë°©ì§€
    
    @staticmethod
    def latency_comparison(tree_latency_ms: float, flat_latency_ms: float) -> Dict[str, float]:
        """
        Latency Comparison: TreeRAG vs Flat RAG ì‘ë‹µì†ë„ ë¹„êµ
        
        Args:
            tree_latency_ms: TreeRAG ì‘ë‹µì‹œê°„ (ë°€ë¦¬ì´ˆ)
            flat_latency_ms: Flat RAG ì‘ë‹µì‹œê°„ (ë°€ë¦¬ì´ˆ)
            
        Returns:
            {
                'tree_ms': TreeRAG ì‹œê°„,
                'flat_ms': Flat RAG ì‹œê°„,
                'speedup': ë°°ì† (>1ì´ë©´ TreeRAGê°€ ë¹ ë¦„),
                'difference_ms': ì°¨ì´ (ms)
            }
        """
        speedup = flat_latency_ms / tree_latency_ms if tree_latency_ms > 0 else 0.0
        
        return {
            'tree_ms': tree_latency_ms,
            'flat_ms': flat_latency_ms,
            'speedup': speedup,
            'difference_ms': flat_latency_ms - tree_latency_ms,
            'faster_system': 'TreeRAG' if speedup > 1.0 else 'FlatRAG'
        }
    
    @staticmethod
    def faithfulness_score(answer: str, source_contexts: List[str], threshold: float = 0.5) -> Dict[str, Any]:
        """
        Faithfulness Score: ë‹µë³€ì´ ì†ŒìŠ¤ì— ì¶©ì‹¤í•œê°€?
        
        Hallucination ê°ì§€ - ê¸°ì¡´ hallucination_detector í™œìš©
        
        Args:
            answer: ìƒì„±ëœ ë‹µë³€
            source_contexts: ì†ŒìŠ¤ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë“¤
            threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
            
        Returns:
            {
                'score': 0.0~1.0,
                'faithful': boolean,
                'low_confidence_sentences': List[str]
            }
        """
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = [s.strip() for s in answer.split('.') if s.strip()]
        
        if not sentences:
            return {
                'score': 1.0,
                'faithful': True,
                'low_confidence_sentences': []
            }
        
        # ê° ë¬¸ì¥ë³„ ì‹ ë¢°ë„ ê³„ì‚° (ê°„ë‹¨í•œ word overlap ê¸°ë°˜)
        sentence_scores = []
        low_confidence = []
        
        for sentence in sentences:
            # ë¬¸ì¥ì˜ ë‹¨ì–´ë“¤ì´ ì†ŒìŠ¤ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ ì¡´ì¬í•˜ëŠ”ê°€?
            sentence_words = set(sentence.lower().split())
            
            max_overlap = 0.0
            for context in source_contexts:
                context_words = set(context.lower().split())
                overlap = len(sentence_words.intersection(context_words))
                overlap_ratio = overlap / len(sentence_words) if sentence_words else 0.0
                max_overlap = max(max_overlap, overlap_ratio)
            
            sentence_scores.append(max_overlap)
            
            if max_overlap < threshold:
                low_confidence.append(sentence)
        
        # ì „ì²´ faithfulness score
        avg_score = np.mean(sentence_scores) if sentence_scores else 0.0
        
        return {
            'score': float(avg_score),
            'faithful': avg_score >= threshold,
            'low_confidence_sentences': low_confidence,
            'sentence_count': len(sentences),
            'low_confidence_count': len(low_confidence)
        }
    
    @staticmethod
    def aggregate_metrics(results: List[Dict[str, float]]) -> Dict[str, Dict[str, float]]:
        """
        ì—¬ëŸ¬ ì¿¼ë¦¬ì˜ ê²°ê³¼ë¥¼ ì§‘ê³„í•˜ì—¬ í‰ê· , í‘œì¤€í¸ì°¨ ê³„ì‚°
        
        Args:
            results: ê° ì¿¼ë¦¬ë³„ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            {
                'metric_name': {
                    'mean': float,
                    'std': float,
                    'min': float,
                    'max': float
                }
            }
        """
        if not results:
            return {}
        
        # ëª¨ë“  ë©”íŠ¸ë¦­ ì´ë¦„ ìˆ˜ì§‘
        all_metrics = set()
        for result in results:
            all_metrics.update(result.keys())
        
        # ê° ë©”íŠ¸ë¦­ë³„ í†µê³„ ê³„ì‚°
        aggregated = {}
        
        for metric in all_metrics:
            values = [r[metric] for r in results if metric in r and isinstance(r[metric], (int, float))]
            
            if values:
                aggregated[metric] = {
                    'mean': float(np.mean(values)),
                    'std': float(np.std(values)),
                    'min': float(np.min(values)),
                    'max': float(np.max(values)),
                    'count': len(values)
                }
        
        return aggregated
    
    @staticmethod
    def comprehensive_report(
        retrieval_metrics: Dict[str, float],
        generation_metrics: Dict[str, Any],
        efficiency_metrics: Dict[str, float]
    ) -> str:
        """
        ì¢…í•© í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±
        
        Args:
            retrieval_metrics: Precision, Recall, F1, NDCG ë“±
            generation_metrics: Faithfulness, Citation accuracy ë“±
            efficiency_metrics: Latency, Context reduction ë“±
            
        Returns:
            ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•íƒœì˜ ì¢…í•© ë¦¬í¬íŠ¸
        """
        report = []
        report.append("=" * 60)
        report.append("TreeRAG Comprehensive Evaluation Report")
        report.append("=" * 60)
        report.append("")
        
        # Retrieval Quality
        report.append("ğŸ“Š Retrieval Quality:")
        report.append("-" * 60)
        for metric, value in retrieval_metrics.items():
            report.append(f"  {metric:30s}: {value:.4f}")
        report.append("")
        
        # Generation Quality
        report.append("âœï¸  Generation Quality:")
        report.append("-" * 60)
        for metric, value in generation_metrics.items():
            if isinstance(value, dict):
                report.append(f"  {metric}:")
                for k, v in value.items():
                    report.append(f"    {k:28s}: {v}")
            else:
                report.append(f"  {metric:30s}: {value}")
        report.append("")
        
        # Efficiency
        report.append("âš¡ Efficiency:")
        report.append("-" * 60)
        for metric, value in efficiency_metrics.items():
            if isinstance(value, (int, float)):
                report.append(f"  {metric:30s}: {value:.2f}")
            else:
                report.append(f"  {metric:30s}: {value}")
        report.append("")
        
        report.append("=" * 60)
        
        return "\n".join(report)
