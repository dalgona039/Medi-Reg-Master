import pytest
import numpy as np
from pathlib import Path
import sys

current_dir = Path(__file__).parent.parent
sys.path.insert(0, str(current_dir))

from src.core.retrieval_model import (
    HierarchicalRetrievalModel,
    RelevanceWeights
)


class TestRelevanceWeights:
    """ê°€ì¤‘ì¹˜ ì„¤ì • í…ŒìŠ¤íŠ¸"""
    
    def test_default_weights(self):
        """ê¸°ë³¸ ê°€ì¤‘ì¹˜ í•© ê²€ì¦"""
        weights = RelevanceWeights()
        
        assert weights.semantic_weight == 0.7
        assert weights.structural_weight == 0.2
        assert weights.contextual_weight == 0.1
        
        # í•©ì€ 1.0
        total = weights.semantic_weight + weights.structural_weight + weights.contextual_weight
        assert abs(total - 1.0) < 1e-6
    
    def test_custom_weights(self):
        """ì‚¬ìš©ì ì •ì˜ ê°€ì¤‘ì¹˜"""
        weights = RelevanceWeights(
            semantic_weight=0.8,
            structural_weight=0.15,
            contextual_weight=0.05
        )
        
        assert weights.semantic_weight == 0.8
        
        total = weights.semantic_weight + weights.structural_weight + weights.contextual_weight
        assert abs(total - 1.0) < 1e-6
    
    def test_invalid_weights(self):
        """ì˜ëª»ëœ ê°€ì¤‘ì¹˜ (í•©ì´ 1.0ì´ ì•„ë‹Œ ê²½ìš°)"""
        with pytest.raises(ValueError, match="must sum to 1.0"):
            RelevanceWeights(
                semantic_weight=0.5,
                structural_weight=0.3,
                contextual_weight=0.3  # í•©=1.1 (ì˜¤ë¥˜)
            )


class TestHierarchicalRetrievalModel:
    """Formal Retrieval Model í…ŒìŠ¤íŠ¸"""
    
    def setup_method(self):
        """ê° í…ŒìŠ¤íŠ¸ ì „ ì‹¤í–‰"""
        self.model = HierarchicalRetrievalModel()
        
        # ìƒ˜í”Œ ë…¸ë“œ
        self.sample_node = {
            'id': 'node1',
            'title': 'í˜ˆë‹¹ ì¡°ì ˆ ì•½ë¬¼',
            'summary': 'ì¸ìŠë¦°ê³¼ ë©”íŠ¸í¬ë¥´ë¯¼ì€ ë‹¹ë‡¨ë³‘ ì¹˜ë£Œì— ì‚¬ìš©ë˜ëŠ” ì£¼ìš” ì•½ë¬¼ì…ë‹ˆë‹¤.',
            'page_ref': 'p.10'
        }
        
        self.sample_parent = {
            'id': 'root',
            'title': 'ë‹¹ë‡¨ë³‘ ì¹˜ë£Œ',
            'summary': 'ë‹¹ë‡¨ë³‘ì˜ ë‹¤ì–‘í•œ ì¹˜ë£Œ ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.'
        }
        
        self.sample_query = "ì¸ìŠë¦° ì¹˜ë£Œ ë°©ë²•ì€?"
    
    def test_model_initialization(self):
        """ëª¨ë¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        model = HierarchicalRetrievalModel()
        
        assert model.weights.semantic_weight == 0.7
        assert model.max_depth == 5
        assert model.depth_decay == 0.9
    
    def test_semantic_relevance(self):
        """ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        score = self.model._semantic_relevance(
            self.sample_node,
            self.sample_query,
            None
        )
        
        # 0~1 ë²”ìœ„ í™•ì¸
        assert 0.0 <= score <= 1.0
        
        # 'ì¸ìŠë¦°'ê³¼ 'ì¹˜ë£Œ' í‚¤ì›Œë“œê°€ ê²¹ì¹˜ë¯€ë¡œ 0 ì´ìƒ
        # (í•œê¸€ í† í°í™” ì´ìŠˆë¡œ ì¸í•´ 0ì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ >= 0.0ë¡œ ìˆ˜ì •)
        assert score >= 0.0
        
        print(f"Semantic relevance: {score:.4f}")
    
    def test_structural_relevance(self):
        score_d0 = self.model._structural_relevance(0)
        assert score_d0 == 1.0
        
        score_d1 = self.model._structural_relevance(1)
        assert abs(score_d1 - 0.9) < 1e-6
        
        score_d2 = self.model._structural_relevance(2)
        assert abs(score_d2 - 0.81) < 1e-6
        
        score_d_max = self.model._structural_relevance(self.model.max_depth)
        assert score_d_max == 0.0
        
        print(f"Structural scores: d=0â†’{score_d0}, d=1â†’{score_d1}, d=2â†’{score_d2}")
    
    def test_contextual_relevance(self):
        score_root = self.model._contextual_relevance(self.sample_node, None)
        assert score_root == 1.0
        
        score_with_parent = self.model._contextual_relevance(
            self.sample_node,
            self.sample_parent
        )
        
        assert 0.0 <= score_with_parent <= 1.0
        assert score_with_parent >= 0.3
        
        print(f"Contextual scores: rootâ†’{score_root}, with_parentâ†’{score_with_parent}")
    
    def test_relevance_score_calculation(self):
        """ì „ì²´ relevance score ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        score, components = self.model.relevance_score(
            node=self.sample_node,
            query=self.sample_query,
            current_depth=1,
            parent_node=self.sample_parent
        )
        
        # ScoreëŠ” 0~1 ë²”ìœ„
        assert 0.0 <= score <= 1.0
        
        # Components í™•ì¸
        assert 'semantic' in components
        assert 'structural' in components
        assert 'contextual' in components
        assert 'total' in components
        
        # Total score ê²€ì‚°
        expected_total = (
            self.model.weights.semantic_weight * components['semantic'] +
            self.model.weights.structural_weight * components['structural'] +
            self.model.weights.contextual_weight * components['contextual']
        )
        
        assert abs(score - expected_total) < 1e-5
        
        print(f"\nRelevance Score Breakdown:")
        print(f"  Semantic:    {components['semantic']:.4f} (weight: {self.model.weights.semantic_weight})")
        print(f"  Structural:  {components['structural']:.4f} (weight: {self.model.weights.structural_weight})")
        print(f"  Contextual:  {components['contextual']:.4f} (weight: {self.model.weights.contextual_weight})")
        print(f"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"  Total Score: {score:.4f}")
    
    def test_rank_nodes(self):
        """ë…¸ë“œ ë­í‚¹ í…ŒìŠ¤íŠ¸"""
        nodes = [
            {'id': 'n1', 'title': 'ì¸ìŠë¦° ì¹˜ë£Œ', 'summary': 'ì¸ìŠë¦° ì£¼ì‚¬ ë°©ë²•'},
            {'id': 'n2', 'title': 'ì‹ì´ìš”ë²•', 'summary': 'ë‹¹ë‡¨ë³‘ í™˜ìë¥¼ ìœ„í•œ ì‹ë‹¨'},
            {'id': 'n3', 'title': 'ì¸ìŠë¦° ì €í•­ì„±', 'summary': 'ì¸ìŠë¦° ì €í•­ì„±ì˜ ì›ì¸ê³¼ ì¹˜ë£Œ'},
        ]
        
        ranked = self.model.rank_nodes(
            nodes=nodes,
            query="ì¸ìŠë¦° ì¹˜ë£Œ ë°©ë²•",
            current_depth=1
        )
        
        # 3ê°œ ë…¸ë“œ ë°˜í™˜
        assert len(ranked) == 3
        
        # ê° ìš”ì†ŒëŠ” (node, score, components) íŠœí”Œ
        for node, score, components in ranked:
            assert 'id' in node
            assert 0.0 <= score <= 1.0
            assert 'semantic' in components
        
        # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í™•ì¸
        scores = [score for _, score, _ in ranked]
        assert scores == sorted(scores, reverse=True)
        
        # 'ì¸ìŠë¦° ì¹˜ë£Œ'ê°€ ê°€ì¥ ë†’ì€ ì ìˆ˜
        top_node = ranked[0][0]
        print(f"\nTop ranked node: {top_node['title']} (score: {ranked[0][1]:.4f})")
        assert 'ì¸ìŠë¦°' in top_node['title'] or 'ì¹˜ë£Œ' in top_node['title']
    
    def test_complexity_analysis(self):
        """ë³µì¡ë„ ë¶„ì„ ì •ë³´ í…ŒìŠ¤íŠ¸"""
        analysis = self.model.get_complexity_analysis()
        
        assert 'time_complexity' in analysis
        assert 'space_complexity' in analysis
        assert 'optimality' in analysis
        
        print(f"\nComplexity Analysis:")
        for key, value in analysis.items():
            print(f"  {key}: {value}")
    
    def test_explain_decision(self):
        """ê²°ì • ì„¤ëª… ìƒì„± í…ŒìŠ¤íŠ¸"""
        explanation = self.model.explain_decision(
            node=self.sample_node,
            query=self.sample_query,
            current_depth=1,
            parent_node=self.sample_parent
        )
        
        assert isinstance(explanation, str)
        assert len(explanation) > 0
        
        # ì£¼ìš” í‚¤ì›Œë“œ í¬í•¨ í™•ì¸
        assert 'Semantic' in explanation
        assert 'Structural' in explanation
        assert 'Contextual' in explanation
        assert 'Final Score' in explanation
        
        print(explanation)
    
    def test_different_weight_configurations(self):
        """ë‹¤ì–‘í•œ ê°€ì¤‘ì¹˜ ì„¤ì • ë¹„êµ"""
        # Configuration 1: Semantic ì¤‘ì‹¬ (ê¸°ë³¸)
        model1 = HierarchicalRetrievalModel(
            weights=RelevanceWeights(0.7, 0.2, 0.1)
        )
        
        # Configuration 2: Structural ì¤‘ì‹¬
        model2 = HierarchicalRetrievalModel(
            weights=RelevanceWeights(0.3, 0.6, 0.1)
        )
        
        # Configuration 3: Balanced
        model3 = HierarchicalRetrievalModel(
            weights=RelevanceWeights(0.5, 0.25, 0.25)
        )
        
        # ê°™ì€ ë…¸ë“œì— ëŒ€í•œ ì ìˆ˜ ë¹„êµ
        node = self.sample_node
        query = self.sample_query
        
        score1, _ = model1.relevance_score(node, query, 1, self.sample_parent)
        score2, _ = model2.relevance_score(node, query, 1, self.sample_parent)
        score3, _ = model3.relevance_score(node, query, 1, self.sample_parent)
        
        print(f"\nScore comparison for different weight configurations:")
        print(f"  Semantic-focused (0.7,0.2,0.1): {score1:.4f}")
        print(f"  Structural-focused (0.3,0.6,0.1): {score2:.4f}")
        print(f"  Balanced (0.5,0.25,0.25): {score3:.4f}")
        
        # ëª¨ë“  ì ìˆ˜ëŠ” 0~1 ë²”ìœ„
        assert 0.0 <= score1 <= 1.0
        assert 0.0 <= score2 <= 1.0
        assert 0.0 <= score3 <= 1.0


class TestFormalModelMathematicalProperties:
    """ìˆ˜í•™ì  íŠ¹ì„± ê²€ì¦"""
    
    def test_score_range_preservation(self):
        """ì ìˆ˜ê°€ í•­ìƒ [0,1] ë²”ìœ„ ìœ ì§€"""
        model = HierarchicalRetrievalModel()
        
        test_cases = [
            # (node, query, depth, parent)
            ({'title': 'A', 'summary': 'B'}, 'C', 0, None),
            ({'title': 'Test', 'summary': 'Test test'}, 'Test', 2, {'title': 'Parent'}),
            ({'title': '', 'summary': ''}, 'query', 3, None),  # Empty node
        ]
        
        for node, query, depth, parent in test_cases:
            score, _ = model.relevance_score(node, query, depth, parent)
            assert 0.0 <= score <= 1.0, f"Score {score} out of range for {node['title']}"
    
    def test_weight_impact(self):
        """ê°€ì¤‘ì¹˜ ë³€í™”ê°€ ì ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥"""
        node = {'title': 'Test', 'summary': 'content'}
        query = 'Test'
        
        # Semantic weight ì¦ê°€ â†’ semantic componentì˜ ì˜í–¥ë ¥ ì¦ê°€
        model_high_semantic = HierarchicalRetrievalModel(
            weights=RelevanceWeights(0.9, 0.05, 0.05)
        )
        
        model_low_semantic = HierarchicalRetrievalModel(
            weights=RelevanceWeights(0.3, 0.5, 0.2)
        )
        
        score_high, comp_high = model_high_semantic.relevance_score(node, query, 1, None)
        score_low, comp_low = model_low_semantic.relevance_score(node, query, 1, None)
        
        print(f"\nWeight impact test:")
        print(f"  High semantic weight (0.9): score={score_high:.4f}")
        print(f"  Low semantic weight (0.3): score={score_low:.4f}")
        
        # Semantic componentê°€ ê°™ë‹¤ë©´, high semantic weightê°€ ë” ë†’ì€ total score
        if comp_high['semantic'] > 0.5:  # semanticì´ ë†’ì€ ê²½ìš°
            assert score_high >= score_low
    
    def test_monotonicity_depth_penalty(self):
        """ê¹Šì´ ì¦ê°€ â†’ structural score ë‹¨ì¡°ê°ì†Œ"""
        model = HierarchicalRetrievalModel(depth_decay=0.9)
        
        scores = []
        for depth in range(model.max_depth):
            score = model._structural_relevance(depth)
            scores.append(score)
        
        print(f"\nDepth penalty monotonicity:")
        for d, s in enumerate(scores):
            print(f"  depth={d}: structural_score={s:.4f}")
        
        # ë‹¨ì¡°ê°ì†Œ í™•ì¸
        for i in range(len(scores) - 1):
            assert scores[i] >= scores[i+1], f"Not monotonic at depth {i}"


def test_formal_model_integration():
    """ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸ”¬ PHASE 1-2: Formal Retrieval Model Integration Test")
    print("="*60 + "\n")
    
    model = HierarchicalRetrievalModel()
    
    # ê°€ìƒì˜ ì‹œë‚˜ë¦¬ì˜¤: ì˜ë£Œ ë¬¸ì„œ íŠ¸ë¦¬
    nodes = [
        {'id': 'n1', 'title': 'ì‹¬í˜ˆê´€ ì§ˆí™˜', 'summary': 'ì‹¬ì¥ê³¼ í˜ˆê´€ ê´€ë ¨ ì§ˆë³‘'},
        {'id': 'n2', 'title': 'ê³ í˜ˆì•• ì¹˜ë£Œ', 'summary': 'í˜ˆì•• ê°•í•˜ì œì™€ ìƒí™œìŠµê´€ ê°œì„ '},
        {'id': 'n3', 'title': 'ë‹¹ë‡¨ë³‘ ê´€ë¦¬', 'summary': 'í˜ˆë‹¹ ì¡°ì ˆê³¼ í•©ë³‘ì¦ ì˜ˆë°©'},
        {'id': 'n4', 'title': 'ê³ í˜ˆì•• ì•½ë¬¼', 'summary': 'ACE ì–µì œì œ, ë² íƒ€ì°¨ë‹¨ì œ ë“±ì˜ í˜ˆì•• ì•½'},
    ]
    
    query = "ê³ í˜ˆì•• ì¹˜ë£Œ ì•½ë¬¼ì€?"
    
    print(f"Query: {query}\n")
    
    ranked = model.rank_nodes(nodes, query, current_depth=1)
    
    print("Ranked Results:")
    print("-" * 60)
    for i, (node, score, components) in enumerate(ranked, 1):
        print(f"\n{i}. {node['title']} (score: {score:.4f})")
        print(f"   Semantic: {components['semantic']:.4f}, "
              f"Structural: {components['structural']:.4f}, "
              f"Contextual: {components['contextual']:.4f}")
    
    print("\n" + "="*60)
    print("âœ… Formal model successfully ranks nodes by relevance!")
    print("="*60)


if __name__ == "__main__":
    test_formal_model_integration()
